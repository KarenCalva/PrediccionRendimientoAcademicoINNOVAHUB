---
output: 
  html_document:
    css: Estilos.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<center> <h1> <b> Librerías utilizadas </b> </h1> </center>

La librería en **R** utilizada para la estimación del modelo implementado en inferencia fue **GBM**, elaborada por Greg Ridgeway (2019). 

<div class="definition_box">
<i> Esta librería implementa las extensiones del algoritmo AdaBoost de Freund y Schapire's y del gradient boosting machine de Friedman. Incluye métodos de regresión por mínimos cuadrados, pérdida absoluta, regresión cuantílica, logística, multinomial logística, Poisson, Cox, pérdida exponencial Adaboost, entre otras.
</div>

<center> <h1> <b> Código utilizado </b> </h1> </center>

La función implementada fue la siguiente:

```{r eval=FALSE}
rm(list=ls())
options(java.parameters="-Xmx8000m")
memory.size(max=T)

#################################################################
#### LIBRERIAS NECESARIAS   
#################################################################

require(gbm)          
require(ggplot2)      
require(e1071)
require(dplyr)
require(InformationValue)
require(ROCR)
require(pROC)
require(parallel)
require(sqldf)
require(purrr)

#################################################################
#### CARGANDO BASE ORIGINAL DE TRABAJO
#################################################################

wd <- "Direccion donde esta la base"
setwd(wd)
load("BaseCursoNivelacion.RData" )

PeriodosEntrenamiento = c("2017-A","2017-B","2018-A","2018-B")
PeriodoPrueba= "2019-A"

Train = BaseCursoNivelacion %>% filter(Periodo %in% PeriodosEntrenamiento)

Prueba = BaseCursoNivelacion %>% filter(Periodo == PeriodoPrueba)

#################################################################
#### ENCONTRAR LOS PARÁMETROS ÓPTIMOS
#################################################################

MatrizParametros <- expand.grid(
  shrinkage = c(.01, .1, .3),
  interaction.depth = c(1, 3, 5),
  n.minobsinnode = c(5, 10, 15),
  bag.fraction = c(.65, .8, 1), 
  optimal_trees = 0,               
  min_RMSE = 0                     
)

# numero total de combinaciones
nrow(MatrizParametros)
## [1] 81

# Busqueda exhaustiva
for(i in 1:nrow(MatrizParametros)) {
  
  # reproducibilidad
  set.seed(123)
  
  # entrenamiento del modelo
  gbm.tune <- gbm(
    formula = Reprueba ~ .,
    distribution = "bernoulli",
    data = Train,
    n.trees = 10000,
    interaction.depth = MatrizParametros$interaction.depth[i],
    shrinkage = MatrizParametros$shrinkage[i],
    n.minobsinnode = MatrizParametros$n.minobsinnode[i],
    bag.fraction = MatrizParametros$bag.fraction[i],
    train.fraction = .7,
    n.cores = NULL, # usará todos los cores disponibles
    verbose = TRUE,
    keep.data = FALSE
  )
  
  MatrizParametros$optimal_trees[i] <- which.min(gbm.tune$valid.error)
  MatrizParametros$min_RMSE[i] <- sqrt(min(gbm.tune$valid.error))
}

MatrizParametros %>% 
  dplyr::arrange(min_RMSE) %>%
  head(1)
  
#################################################################
#### ENTRENAMIENTO DEL MODELO GBM
#################################################################

set.seed(123)

# train GBM model
gbm.fit.final <- gbm(
  formula = Reprueba ~ .,
  distribution = "bernoulli",
  data = Train,
  n.trees = 10000,
  interaction.depth = 5,
  shrinkage = 0.01,
  n.minobsinnode = 15,
  bag.fraction = .85, 
  train.fraction = 1,
  n.cores = NULL, 
  verbose = FALSE,
  keep.data = FALSE,
  cv.folds=5
  )  

print(gbm.fit)
```

<span style="padding-left:15px">

<center><img src="figs/runR.jfif" width="50%"></center>

<span style="padding-left:15px">

[Resultados del modelo](https://karencalva.shinyapps.io/CursoNivelacionEPN/)